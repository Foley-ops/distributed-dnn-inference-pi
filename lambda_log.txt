2025-04-11 08:34:50,913 - INFO - Starting distributed inference process [PlamaLV:rank0]
2025-04-11 08:34:50,914 - INFO - Using master address: 10.100.117.1 and port: 55555 [PlamaLV:rank0]
2025-04-11 08:34:50,914 - INFO - Set GLOO_SOCKET_IFNAME to enp6s0 for binding [PlamaLV:rank0]
2025-04-11 08:34:50,914 - INFO - Initializing master node [PlamaLV:rank0]
2025-04-11 08:34:50,914 - INFO - Master using explicit init_method: tcp://0.0.0.0:55555 [PlamaLV:rank0]
/home/durga/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:146: UserWarning: 
NVIDIA GeForce RTX 4090 with CUDA capability sm_89 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the NVIDIA GeForce RTX 4090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
[W DispatchStub.cpp:37] Warning: ignoring invalid value for ATEN_CPU_CAPABILITY:  (function compute_cpu_capability)
2025-04-11 08:34:52,003 - INFO - Master RPC initialized successfully [PlamaLV:rank0]
2025-04-11 08:34:52,003 - INFO - Setting up model with workers: ['worker1', 'worker2'] [PlamaLV:rank0]
2025-04-11 08:34:52,004 - INFO - Distributed model created successfully [PlamaLV:rank0]
2025-04-11 08:34:52,004 - INFO - Loading cifar10 dataset [PlamaLV:rank0]
2025-04-11 08:34:52,004 - INFO - Loading CIFAR-10 from: /home/durga/datasets/cifar10 [PlamaLV:rank0]
2025-04-11 08:34:52,256 - INFO - Loaded batch of 1 images with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:34:52,256 - INFO - First few labels: tensor([8]) [PlamaLV:rank0]
2025-04-11 08:34:52,256 - INFO - Starting inference... [PlamaLV:rank0]
2025-04-11 08:34:52,256 - INFO - Running inference on batch 1/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:34:52,256 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:16,403 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:16,404 - INFO - Predicted: tensor([7]) | Actual: tensor([7]) [PlamaLV:rank0]
2025-04-11 08:35:16,404 - INFO - Running inference on batch 2/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:16,404 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:19,050 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:19,050 - INFO - Predicted: tensor([5]) | Actual: tensor([5]) [PlamaLV:rank0]
2025-04-11 08:35:19,051 - INFO - Running inference on batch 3/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:19,051 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:21,710 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:21,710 - INFO - Predicted: tensor([3]) | Actual: tensor([5]) [PlamaLV:rank0]
2025-04-11 08:35:21,711 - INFO - Running inference on batch 4/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:21,711 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:24,210 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:24,210 - INFO - Predicted: tensor([2]) | Actual: tensor([2]) [PlamaLV:rank0]
2025-04-11 08:35:24,211 - INFO - Running inference on batch 5/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:24,211 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:26,897 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:26,897 - INFO - Predicted: tensor([9]) | Actual: tensor([9]) [PlamaLV:rank0]
2025-04-11 08:35:26,898 - INFO - Running inference on batch 6/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:26,898 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:29,880 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:29,881 - INFO - Predicted: tensor([2]) | Actual: tensor([5]) [PlamaLV:rank0]
2025-04-11 08:35:29,881 - INFO - Running inference on batch 7/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:29,881 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:32,642 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:32,643 - INFO - Predicted: tensor([0]) | Actual: tensor([0]) [PlamaLV:rank0]
2025-04-11 08:35:32,643 - INFO - Running inference on batch 8/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:32,643 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:35,939 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:35,940 - INFO - Predicted: tensor([2]) | Actual: tensor([2]) [PlamaLV:rank0]
2025-04-11 08:35:35,940 - INFO - Running inference on batch 9/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:35,940 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:38,520 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:38,520 - INFO - Predicted: tensor([7]) | Actual: tensor([7]) [PlamaLV:rank0]
2025-04-11 08:35:38,521 - INFO - Running inference on batch 10/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:38,521 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:41,821 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:41,822 - INFO - Predicted: tensor([1]) | Actual: tensor([1]) [PlamaLV:rank0]
2025-04-11 08:35:41,822 - INFO - Running inference on batch 11/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:41,822 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:45,446 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:45,446 - INFO - Predicted: tensor([8]) | Actual: tensor([8]) [PlamaLV:rank0]
2025-04-11 08:35:45,447 - INFO - Running inference on batch 12/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:45,447 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:48,163 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:48,163 - INFO - Predicted: tensor([9]) | Actual: tensor([9]) [PlamaLV:rank0]
2025-04-11 08:35:48,164 - INFO - Running inference on batch 13/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:48,164 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:51,309 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:51,310 - INFO - Predicted: tensor([2]) | Actual: tensor([2]) [PlamaLV:rank0]
2025-04-11 08:35:51,310 - INFO - Running inference on batch 14/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:51,310 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:54,374 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:54,374 - INFO - Predicted: tensor([7]) | Actual: tensor([7]) [PlamaLV:rank0]
2025-04-11 08:35:54,375 - INFO - Running inference on batch 15/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:54,375 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:35:57,225 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:35:57,226 - INFO - Predicted: tensor([8]) | Actual: tensor([8]) [PlamaLV:rank0]
2025-04-11 08:35:57,226 - INFO - Running inference on batch 16/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:35:57,226 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:36:00,626 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:36:00,626 - INFO - Predicted: tensor([9]) | Actual: tensor([9]) [PlamaLV:rank0]
2025-04-11 08:36:00,627 - INFO - Running inference on batch 17/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:36:00,627 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:36:03,134 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:36:03,134 - INFO - Predicted: tensor([0]) | Actual: tensor([0]) [PlamaLV:rank0]
2025-04-11 08:36:03,135 - INFO - Running inference on batch 18/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:36:03,135 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:36:06,090 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:36:06,090 - INFO - Predicted: tensor([0]) | Actual: tensor([0]) [PlamaLV:rank0]
2025-04-11 08:36:06,091 - INFO - Running inference on batch 19/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:36:06,091 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
2025-04-11 08:36:08,787 - INFO - Received output shape: torch.Size([1, 10]) [PlamaLV:rank0]
2025-04-11 08:36:08,787 - INFO - Predicted: tensor([2]) | Actual: tensor([0]) [PlamaLV:rank0]
2025-04-11 08:36:08,788 - INFO - Running inference on batch 20/100 with shape: torch.Size([1, 3, 224, 224]) [PlamaLV:rank0]
2025-04-11 08:36:08,788 - INFO - Processing micro-batch 1/2 [PlamaLV:rank0]
